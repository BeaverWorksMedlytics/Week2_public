{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sleep_Analysis_Challenge.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "FB6YunD0Gdgc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# \"You Snooze, You Win\" Challenge\n",
        "\n",
        "Every year, the [PhysioNet/CinC (Computing in Cardiology) Challenge](https://www.physionet.org/challenge/) invites \"participants to tackle clinically interesting problems that are either unsolved or not well-solved.\" As you may have figured out from the title, this year's challenge focuses on sleep, particularly the classification of nonarousal and arousal timeframes. If you would like to understand the biological implications of the challenge, we recommend reading PhysioNet's [introduction](https://physionet.org/challenge/2018/) of the challenge.\n",
        "\n",
        "For this challenge, you will classify samples into 5 classes (Arousal, NREM1, NREM2, NREM3, REM). Each sample consists of seven physiological signals (O2-M1, E1-M2, Chin1-Chin2, ABD, CHEST, AIRFLOW, ECG) measured at 200 Hz over a 60 second period (12000 timepoints). In this notebook, we provide code to import the data, visualize sample signals, implement an example classifier, and 'score' your model."
      ]
    },
    {
      "metadata": {
        "id": "7ctSzKHoGr1q",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "### Run once ###\n",
        "\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn import metrics\n",
        "from sklearn.utils import shuffle\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_XxIE6ZwGvjh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Loading the Dataset\n",
        "\n",
        "This dataset is a modified version of the PhysioNet/CinC Challenge data, which were contributed by the Massachusetts General Hospitalâ€™s Computational Clinical Neurophysiology Laboratory, and the Clinical Data Animation Laboratory.\n",
        "***\n",
        "**Class labels:**\n",
        "- 0 = Arousal\n",
        "- 1 = NREM1\n",
        "- 2 = NREM2\n",
        "- 3 = NREM3\n",
        "- 4 = REM\n",
        "***\n",
        "**Class description:**\n",
        "\n",
        "|Class|State|Characterizations|Brain Waves|Percentage of Sleep|\n",
        "|-|-|-|-|-|-|\n",
        "|Arousal|Consciousness|Wakefulness (coherent cognitive and behavioral responses to external stimuli)|Alpha, Beta|-|\n",
        "|NREM1|Light sleep|Hypnic jerk (involuntary twitch that causes an individual to awaken for a moment)|Theta|5|\n",
        "|NREM2|Unequivocal sleep|Sleep spindle (sudden burst of oscillatory brain activity); K-complex (delta wave that lasts for a second)|Theta, Delta|40-50|\n",
        "|NREM3|Deep sleep|Parasomnias (sleep disorders such as sleepwalking and night terrors)|Delta|15-25|\n",
        "|REM|Dreaming sleep|REM atonia (paralysis of nonessential skeletal muscles); Dreaming|Alpha, Beta|20-25|\n",
        "***\n",
        "**Physiological signal description:**\n",
        "\n",
        "O2-M1 - posterior brain activity (electroencephalography)\n",
        "\n",
        "E1-M2 - left eye activity (electrooculography)\n",
        "\n",
        "Chin1-Chin2 - chin movement (electromyography)\n",
        "\n",
        "ABD - abdominal movement (electromyography)\n",
        "\n",
        "CHEST - chest movement (electromyography)\n",
        "\n",
        "AIRFLOW - respiratory airflow\n",
        "\n",
        "ECG - cardiac activity (electrocardiography)\n",
        "***\n",
        "Run both cell blocks to get the challenge data."
      ]
    },
    {
      "metadata": {
        "id": "yfgbZPNYziXt",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/BeaverWorksMedlytics/datasets.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k_iKzgvyzioe",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "os.chdir('./datasets/Week2_Challenge/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qANF2BvQG2m3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data Visualization\n",
        "\n",
        "Run the first cell once to store the training and test sample file locations. If the data have been imported correctly, the cell should output '4000' (training) and '1000' (test).\n",
        "\n",
        "Run the second cell once to initialize important functions that you may find useful. Descriptions of input and output will be provided for every function in the notebook.\n",
        "\n",
        "Run the last cell to visualize a random test sample's seven physiological signals in a raw and FFT (fast fourier transform) format. Note that you can change different parameters, which we will go over in detail.\n",
        "\n",
        "It is important to recognize that the same signal from different samples in the same class may vary in terms of amplitude and frequency. Of course, this is a byproduct of intraspecies variation. Further data preprocessing and/or discriminatory feature extraction may be necessary to account for this phenomenon."
      ]
    },
    {
      "metadata": {
        "id": "-Rw8inOvG5QP",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "### Run once ###\n",
        "\n",
        "def get_file_locs():\n",
        "    file_dict = {'training':[], 'test':[]}\n",
        "    for data_type in file_dict:\n",
        "        for file in os.listdir('./' + data_type):\n",
        "            file_dict[data_type].append(data_type + '/' + file)\n",
        "    \"\"\" file_dict (dict) stores 'training' and 'test' as keys and sample file locations as values \"\"\"\n",
        "    return file_dict\n",
        "\n",
        "file_dict = get_file_locs()\n",
        "print(len(file_dict['training']), len(file_dict['test']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G6x9vTz5HE1R",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "### Run once ###\n",
        "\n",
        "\"\"\" data_type (str) must be 'training' or 'test' \"\"\"\n",
        "def get_random_data(data_type): #\n",
        "    rand_file = random.choice(file_dict[data_type])\n",
        "    rand_sample = pd.read_pickle('./' + rand_file)\n",
        "    \"\"\" rand_sample (pd dataframe) stores the complete sample data; rand_file.split('/')[1] (str) stores the sample number \"\"\"\n",
        "    return rand_sample, rand_file.split('/')[1]\n",
        "\n",
        "\"\"\" data_type same as get_random_data() input; id_number (int) must be from 0-3999 for training or 0-999 for test \"\"\"\n",
        "def get_sample_data(data_type, id_number):\n",
        "    file = './' + data_type + '/' + str(id_number) + '.xz'\n",
        "    sample = pd.read_pickle('./' + file)\n",
        "    \"\"\" sample and file.split('/')[2] same as get_random_data() output \"\"\"\n",
        "    return sample, file.split('/')[2]\n",
        "\n",
        "\"\"\" sample (pd dataframe) must be output from get_random_data() or get_sample_data() \"\"\"\n",
        "def get_raw_signals(sample):\n",
        "    raw_signals_x = np.arange(0, 60, step = 1/200)\n",
        "    raw_signals_y = np.transpose(sample['Signal'][0])\n",
        "    \"\"\" tuple: raw_signals_x (ndarray) stores raw signal timepoints in seconds; raw_signals_y (ndarray) stores all seven signals for every timepoint; 'Raw' (str) stores signal type \"\"\"\n",
        "    return (raw_signals_x, raw_signals_y, 'Raw')\n",
        "\n",
        "\"\"\" sample same as get_raw_signals() input \"\"\"\n",
        "def get_fft_signals(sample):\n",
        "    num_datapoints = len(sample['Signal'][0])\n",
        "    fft_signals_x = np.arange(num_datapoints//2 + 1)/60\n",
        "    fft_signals_y = np.ndarray(shape = (7, num_datapoints//2 + 1))\n",
        "    for counter, signal in enumerate(np.fft.rfft(np.transpose(sample['Signal'][0]))):\n",
        "        fft_signals_y[counter] = np.abs(signal)\n",
        "    \"\"\" tuple: fft_signals_x (ndarray) stores fft signal frequencies in hertz; fft_signals_y (ndarray) stores amplitude for every frequency; 'FFT' stores signal type \"\"\"\n",
        "    return (fft_signals_x, fft_signals_y, 'FFT')\n",
        "\n",
        "\"\"\" sample same as get_raw_signals() input \"\"\"\n",
        "def get_signals(sample):\n",
        "    raw_signals = get_raw_signals(sample)\n",
        "    fft_signals = get_fft_signals(sample)\n",
        "    \"\"\" raw_signals and fft_signals same as get_raw_signals() and get_fft_signals() output, respectively \"\"\"\n",
        "    return raw_signals, fft_signals\n",
        "\n",
        "\"\"\" signal_list (list) contains output (tuple) from get_raw_signals(), get_fft_signals(), or get_signals() \"\"\"\n",
        "def graph_signals(signal_list):\n",
        "    num_type_signals = len(signal_list)\n",
        "    plt.figure(figsize = (20, 18))\n",
        "    for signal_format_index, signal_xy in enumerate(signal_list):\n",
        "        for signal_index in range(7):\n",
        "            plt.subplot(7, num_type_signals, num_type_signals*signal_index + signal_format_index + 1)\n",
        "            if signal_index==0:\n",
        "                plt.title(signal_xy[2])\n",
        "            plt.plot(signal_xy[0], signal_xy[1][signal_index])\n",
        "            plt.ylabel(sig_dict[signal_index])\n",
        "        plt.xlabel(sig_type_dict[signal_format_index])\n",
        "    plt.show()\n",
        "\n",
        "\"\"\" Initalizes key reference dictionaries \"\"\"\n",
        "sig_dict = {0:'O2-M1', 1:'E1-M2', 2:'Chin1-Chin2', 3:'ABD', 4:'CHEST', 5:'AIRFLOW', 6:'ECG'}\n",
        "sig_type_dict = {0:'Time (s)', 1:'Frequency (Hz)'}\n",
        "stage_dict = {0:'Arousal', 1:'NREM1', 2:'NREM2', 3:'NREM3', 4:'REM'}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zjgpE5L3Chco",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "data_type = 'test'\n",
        "sample, sample_id = get_random_data(data_type)\n",
        "#sample, sample_id = get_sample_data(data_type, 0)\n",
        "raw_signals, fft_signals = get_signals(sample)\n",
        "if data_type == 'training': \n",
        "    print(data_type.title(), sample_id, '(' + stage_dict[int(sample_id.split('.')[0])//800] + ')')\n",
        "else: \n",
        "    print(data_type.title(), sample_id, '(Unknown)')\n",
        "graph_signals([raw_signals, fft_signals])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mXsZa50WHGZR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Example Classifier\n",
        "\n",
        "Below is an example of a poor classifier for this dataset. It is a simple neural network that takes the maximal, normalized variance for each signal (given a frame and interval) as its input features. After training, it performs with 44% accuracy on test data.\n",
        "\n",
        "While the example classifier makes use of a neural network, we encourage you to utilize any ML algorithm that you feel would be appropriate. Also, note that this example classifier makes use of only the raw signals, without consideration of the FFT signals (or any other processed form of the raw signals)."
      ]
    },
    {
      "metadata": {
        "id": "RUhVo7jkHIdS",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "### Run every time you modify your feature extraction ###\n",
        "    \n",
        "#\"\"\" signals (ndarray) must be from index 1 of get_raw_signals(), get_fft_signals(), or get_signals(); frame (int) must be from 0-12000; interval (int) must be from 0-12000 \"\"\"\n",
        "def get_partial_features(signals, frame, interval):\n",
        "    partial_array = np.ndarray(shape = (signals.shape[0], (signals.shape[1]-frame)//interval+1))\n",
        "    for signal_index, signal in enumerate(signals):\n",
        "        for partial_index in range((len(signal)-frame)//interval+1):\n",
        "            \n",
        "            \"\"\" Change feature extraction here \"\"\"\n",
        "            partial_array[signal_index][partial_index] = np.var(signal[(interval*partial_index):(interval*partial_index)+frame])\n",
        "     \n",
        "    \"\"\" partial_array (ndarray) stores multiple features for each signal\"\"\"\n",
        "    return partial_array\n",
        "\n",
        "\"\"\" signals same as get_partial_features() input \"\"\"\n",
        "def consolidate_features(signals):\n",
        "    total_array = np.ndarray(shape = (signals.shape[0], 1))\n",
        "    for signal_index, signal in enumerate(signals):\n",
        "        \n",
        "        \"\"\" Change feature extraction here \"\"\"\n",
        "        if np.sum(signal)>0:\n",
        "            total_array[signal_index] = int(np.max(signal)/np.sum(signal)*100)\n",
        "        else:\n",
        "            total_array[signal_index] = 0\n",
        "            \n",
        "    \"\"\" total_array (ndarray) stores single feature for each signal \"\"\"\n",
        "    return total_array\n",
        "\n",
        "\"\"\" data_type (str) must be 'training' or 'test'; num_samples (int) must be 0-4000 for 'training' or 0-1000 for 'test' \"\"\"\n",
        "def get_features(data_type, num_samples):\n",
        "    features = np.array([[],[],[],[],[],[],[]])\n",
        "    order = np.array([])\n",
        "    for i in range(num_samples):\n",
        "        sample, sample_id = get_sample_data(data_type, i)\n",
        "        sample_raw = get_raw_signals(sample)\n",
        "        sample_y_partial = get_partial_features(sample_raw[1], 200, 200)\n",
        "        sample_y_consolidate = consolidate_features(sample_y_partial)\n",
        "        features = np.concatenate((features, sample_y_consolidate), axis=1)\n",
        "        order = np.append(order, int(sample_id.split('.')[0]))\n",
        "    \"\"\" np.transpose(features) (ndarray) stores final features for training/testing; order (ndarray) stores order in which samples were retrieved \"\"\"\n",
        "    return np.transpose(features), order"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SSB4-mn-Chct",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "### Run every time you modify your feature extraction ###\n",
        "\n",
        "train_data, train_order = get_features('training', 4000)\n",
        "test_data, test_order = get_features('test', 1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XN0BBD3fHKRq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "### Run once ###\n",
        "\n",
        "train_labels = np.ndarray(shape = (1, 4000))\n",
        "for i in range(4000):\n",
        "    train_labels[0][i] = i//800\n",
        "train_labels = tf.keras.utils.to_categorical(train_labels[0], 5)\n",
        "train_labels = train_labels.astype(np.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kfLOZkTOChcx",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "### Run every time you modify your feature extraction ###\n",
        "train_data = train_data.astype(np.float32)\n",
        "test_data = test_data.astype(np.float32)\n",
        "train_labels = train_labels.astype(np.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NHSR4oinHLw6",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "### Run whenever you want to check or view your data and labels ###\n",
        "\n",
        "print(train_data.shape, train_labels.shape, '\\n\\n', train_data, '\\n\\n', train_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W_0M_8mJHORZ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "### Run every time you modify your feature extraction ###\n",
        "\n",
        "train_data, train_labels = shuffle(train_data, train_labels)\n",
        "\n",
        "\"\"\" split_samples (int) must be from 0-4000 \"\"\"\n",
        "split_samples = int(4000 * 1/3)\n",
        "\n",
        "val_data = train_data[:split_samples]\n",
        "partial_train_data = train_data[split_samples:]\n",
        "val_labels = train_labels[:split_samples]\n",
        "partial_train_labels = train_labels[split_samples:]\n",
        "\n",
        "training_set = tf.data.Dataset.from_tensor_slices((partial_train_data, partial_train_labels))\n",
        "training_set = training_set.shuffle(partial_train_labels.shape[0]).batch(40)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sOcj_uuUHP3Z",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "### Run every time you change your model parameters ###\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "\"\"\" Modify to your heart's and algorithm's content ^_^ \"\"\"\n",
        "model.add(tf.keras.layers.Dense(64, activation=tf.nn.relu, input_shape=(train_data.shape[1],)))\n",
        "model.add(tf.keras.layers.Dense(64, activation=tf.nn.relu))\n",
        "model.add(tf.keras.layers.Dense(5, activation=tf.nn.softmax))\n",
        "model.compile(loss='categorical_crossentropy', optimizer=tf.train.RMSPropOptimizer(learning_rate=0.001), metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lhhsAXXJHRvJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "### Run whenever you want to train and validate your model ###\n",
        "\n",
        "\"\"\" EPOCHS (int) is whatever number causes stabilization of validation loss and accuracy \"\"\"\n",
        "EPOCHS = 50\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    for signals, labels in training_set:\n",
        "        tr_loss, tr_accuracy = model.train_on_batch(signals, labels)\n",
        "    val_loss, val_accuracy = model.evaluate(val_data, val_labels)\n",
        "    print(('Epoch #%d\\t Training Loss: %.2f\\tTraining Accuracy: %.2f\\t'\n",
        "         'Validation Loss: %.2f\\tValidation Accuracy: %.2f')\n",
        "         % (epoch + 1, tr_loss, tr_accuracy,\n",
        "         val_loss, val_accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bduvOuTtHVfK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Submitting Your Model\n",
        "\n",
        "After training your classifier, run it on the test data to generate your predictions. Each class for a test sample should have an associated probability (between 0 and 1). Below are the parameters for the prediction format and export:\n",
        "\n",
        "- Your predictions should be in a pandas dataframe with 5 columns (classes) and 1000 rows (samples). Note that your predictions must follow the original test sample order (0.xz, 1.xz, 2.xz, ...). You only need to worry about this if you shuffled the test samples or stored the samples in an unordered data structure (dictionaries and sets). If this is the case, you should 1) add a separate column in your pandas dataframe with the file number for each sample; 2) sort the dataframe using this column; and 3) drop the column. These steps have been noted in the code below.\n",
        "- The predictions dataframe should be exported as an .xz file using dataframe.to_pickle() followed by files.download().\n",
        "\n",
        "Example code of the prediction format and export is presented in the cell block below. "
      ]
    },
    {
      "metadata": {
        "id": "CRk6Xh_LHTXB",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "### Run once after you have finished training your model ###\n",
        "\n",
        "test_pred = model.predict(test_data)\n",
        "test_output = np.ndarray(shape = (1000, 6))\n",
        "\n",
        "\"\"\" Add column with file number \"\"\"\n",
        "for i in range(1000):\n",
        "    test_output[i] = np.append(test_pred[i], test_order[i])\n",
        "test_dataframe = pd.DataFrame(test_output)\n",
        "\n",
        "\"\"\" Sort dataframe according to file number \"\"\"\n",
        "sorted_test_dataframe = test_dataframe.sort_values(by=[5])\n",
        "\n",
        "\"\"\" Drop file number column \"\"\"\n",
        "processed_test_dataframe = sorted_test_dataframe.drop(sorted_test_dataframe.columns[5], axis=1)\n",
        "\n",
        "print(test_dataframe.head(), '\\n\\n', sorted_test_dataframe.head(), '\\n\\n', processed_test_dataframe.head())\n",
        "\n",
        "file = 'PotatoesAreGreat.xz'\n",
        "processed_test_dataframe.to_pickle(file)\n",
        "files.download(file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8TiEUxHcZz_D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Your model will be evaluated on Area Under the ROC Curve (ROCAUC), Matthews Correlation Coefficient (MCC) and creativity. There will be a \"winning\" group for each of these categories.\n",
        "\n",
        "If you are finished early, consider trying other ML algorithms and/or implementing multiple feature extraction methods. You can also help other groups if you finish early."
      ]
    },
    {
      "metadata": {
        "id": "LDQWbhUK6cMt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## How Your Model Will Be Evaluated\n",
        "\n",
        "- **Area Under the ROC Curve (AUCROC)**: The receiver operating characteristic (ROC) curve plots the true positive rate (sensitivity/recall) against the false positive rate (fall-out) at many decision threshold settings. The area under the curve (AUC) measures discrimination, the classifier's ability to correctly identify samples from the \"positive\" and \"negative\" cases. Intuitively, AUC is the probability that a randomly chosen \"positive\" sample will be labeled as \"more positive\" than a randomly chosen \"negative\" sample. In the case of a multi-class ROC curve, each class is considered separately before taking the weighted average of all the class results. Simply put, the class under consideration is labeled as \"positive\" while all other classes are labeled as \"negative.\" Below is the multi-class ROC curve for the example classifier. The AUCROC score should be between 0 and 1, in which 0.5 is random classification and 1 is perfect classification.\n",
        "\n",
        " ![alt text](https://image.ibb.co/g4pzST/ROCAUC.png)\n",
        "\n",
        "- **Matthews Correlation Coefficient (MCC)**: The MCC measures the quality of binary classifications, irrespective of the class sizes. Importantly, it is typically regarded as a balanced measure since it considers all values in the 2x2 contingency table (TP, FP, TN, FN). For this challenge, the binary classes will be \"Arousal\" (Arousal) and \"Nonarousal\" (NREM1, NREM2, NREM3, REM). The MCC score should be between -1 and 1, in which 0 is random classification and 1 is perfect classification.\n",
        "\n",
        " ![alt text](https://wikimedia.org/api/rest_v1/media/math/render/svg/5caa90fc15105b74b59a30bbc9cc2e5bd43a13b7)\n",
        "\n",
        "Using these metrics, the example classifier has the following scores on test data:\n",
        "- AUCROC: 0.755\n",
        "- MCC: 0.353\n",
        "- Creativity: ( Í¡Â° ÍœÊ– Í¡Â°)\n",
        "\n",
        "Below is the code used to calculate the AUCROC and MCC metrics when evaluating your classifier."
      ]
    },
    {
      "metadata": {
        "id": "vOpioHig8688",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "test_pred = pd.read_pickle('INSERT PREDICTIONS HERE')\n",
        "test_predict = test_pred.idxmax(axis=1)\n",
        "test_labels = pd.read_pickle('INSERT LABELS HERE')\n",
        "test_labels_one_hot = pd.DataFrame(np.eye(5)[test_labels.values.reshape(1000).astype(int)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vIlfwfksLp-j",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "fpr = {}\n",
        "tpr = {}\n",
        "roc_auc = {}\n",
        "for i in range(5):\n",
        "    fpr[i], tpr[i], _ = metrics.roc_curve(test_labels_one_hot.iloc[:, i], test_pred.iloc[:, i])\n",
        "    roc_auc[i] = metrics.auc(fpr[i], tpr[i])\n",
        "    \"\"\"plt.plot(fpr[i], tpr[i], label = stage_dict[i] + ', ' + str(i))\n",
        "plt.plot([0, 1], [0, 1])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Multi-Class ROC Curve')\n",
        "plt.legend()\n",
        "plt.show()\"\"\"\n",
        "\n",
        "fpr[\"micro\"], tpr[\"micro\"], _ = metrics.roc_curve(test_labels_one_hot.values.ravel(), test_pred.values.ravel())\n",
        "roc_auc = metrics.auc(fpr[\"micro\"], tpr[\"micro\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XO1I5lme8oya",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "y_true = []\n",
        "y_pred = []\n",
        "for i in range(1000):\n",
        "    if test_predict.iloc[i]==0: y_pred.append(1)\n",
        "    else: y_pred.append(-1)\n",
        "    if test_labels.iloc[i][0]==0: y_true.append(1)\n",
        "    else: y_true.append(-1)\n",
        "mcc = metrics.matthews_corrcoef(y_true, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}